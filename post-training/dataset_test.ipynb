{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation — OD Documentation Dataset\n",
        "\n",
        "This notebook:\n",
        "1. Parses all `.md` files from `od_docu/garbage/ingestion/` and subfolders\n",
        "2. Cleans text: removes links, images, code blocks, HTML, markdown formatting\n",
        "3. Filters out files with < 100 symbols\n",
        "4. Applies data cleaning (paragraph length filter, repetition filter, deduplication)\n",
        "5. Tokenizes with GPT-2 tokenizer (tiktoken) and packages for post-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "import heapq\n",
        "import numpy as np\n",
        "import datasets\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Parse and Clean Markdown Files\n",
        "\n",
        "### 1.1 Define the cleaning function\n",
        "\n",
        "The cleaning pipeline removes:\n",
        "- Code blocks (fenced with triple backticks)\n",
        "- HTML comments (`<!-- ... -->`)\n",
        "- HTML tags (`<p>`, `<img>`, `<br>`, etc.)\n",
        "- Images (`![alt](url)`)\n",
        "- Markdown links → keep link text only (`[text](url)` → `text`)\n",
        "- Bare URLs (http/https)\n",
        "- Markdown tables\n",
        "- Markdown formatting (headers `#`, bold `**`, italic `_`, blockquotes `>`, horizontal rules `---`)\n",
        "- Template/variable syntax (`{{ ... }}`)\n",
        "- Inline code backticks containing file paths or code (single backtick wrapped code)\n",
        "- Excessive whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_markdown(text: str) -> str:\n",
        "    \"\"\"Clean markdown text by removing links, images, code blocks,\n",
        "    HTML tags, and markdown formatting. Returns plain text.\"\"\"\n",
        "\n",
        "    # 1. Remove fenced code blocks (```...```)\n",
        "    text = re.sub(r'```[\\s\\S]*?```', '', text)\n",
        "\n",
        "    # 2. Remove HTML comments (<!-- ... -->)\n",
        "    text = re.sub(r'<!--[\\s\\S]*?-->', '', text)\n",
        "\n",
        "    # 3. Remove HTML tags (including self-closing and with attributes)\n",
        "    #    e.g. <p align=\"center\">, <img src=\"...\">, <br>, </p>\n",
        "    text = re.sub(r'<[^>]+/?>', '', text)\n",
        "\n",
        "    # 4. Remove images: ![alt text](url)\n",
        "    text = re.sub(r'!\\[[^\\]]*\\]\\([^)]*\\)', '', text)\n",
        "\n",
        "    # 5. Remove template/variable syntax: {{ ... }}\n",
        "    text = re.sub(r'\\{\\{[^}]*\\}\\}', '', text)\n",
        "\n",
        "    # 6. Convert markdown links to just the link text: [text](url) → text\n",
        "    text = re.sub(r'\\[([^\\]]*)\\]\\([^)]*\\)', r'\\1', text)\n",
        "\n",
        "    # 7. Remove bare URLs\n",
        "    text = re.sub(r'https?://[^\\s)>]+', '', text)\n",
        "\n",
        "    # 8. Remove inline code with file paths / technical identifiers\n",
        "    #    (backtick-wrapped content that looks like paths, CSS selectors, variable names)\n",
        "    text = re.sub(r'`[^`]*`', '', text)\n",
        "\n",
        "    # 9. Remove markdown table separators: |---|---|---|\n",
        "    text = re.sub(r'^\\|?[-|: ]+\\|?$', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 10. Remove markdown table rows (lines starting/ending with |)\n",
        "    text = re.sub(r'^\\|.*\\|\\s*$', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 11. Remove markdown heading markers (#)\n",
        "    text = re.sub(r'^#{1,6}\\s*', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 12. Remove blockquote markers (>)\n",
        "    text = re.sub(r'^>\\s?', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 13. Remove horizontal rules (---, ***, ___)\n",
        "    text = re.sub(r'^([-*_])\\1{2,}\\s*$', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 14. Remove bold/italic markers (**, __, *, _) but keep the text\n",
        "    text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)  # **bold**\n",
        "    text = re.sub(r'__([^_]+)__', r'\\1', text)        # __bold__\n",
        "    text = re.sub(r'\\*([^*]+)\\*', r'\\1', text)        # *italic*\n",
        "    text = re.sub(r'_([^_\\s][^_]*)_', r'\\1', text)    # _italic_\n",
        "\n",
        "    # 15. Remove list markers (-, *, numbered)\n",
        "    text = re.sub(r'^\\s*[-*+]\\s+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'^\\s*\\d+\\.\\s+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 16. Remove escaped characters\n",
        "    text = re.sub(r'\\\\([\\\\`*_{}\\[\\]()#+\\-.!>])', r'\\1', text)\n",
        "\n",
        "    # 17. Clean up excessive blank lines (3+ newlines → 2)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "\n",
        "    # 18. Strip leading/trailing whitespace from each line\n",
        "    lines = [line.strip() for line in text.split('\\n')]\n",
        "    text = '\\n'.join(lines)\n",
        "\n",
        "    # 19. Final strip\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Parse all .md files from the ingestion folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 255 markdown files\n"
          ]
        }
      ],
      "source": [
        "# Path to the ingestion documentation folder\n",
        "ingestion_dir = \"./od_docu/\"\n",
        "\n",
        "# Recursively find all .md files\n",
        "md_files = glob.glob(os.path.join(ingestion_dir, \"**/*.md\"), recursive=True)\n",
        "print(f\"Found {len(md_files)} markdown files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kept: 255 files\n",
            "Skipped (raw < 100 symbols): 0\n",
            "Skipped (cleaned < 100 symbols): 0\n"
          ]
        }
      ],
      "source": [
        "# Read and clean all files\n",
        "MIN_SYMBOLS = 100\n",
        "\n",
        "raw_texts = []\n",
        "cleaned_texts = []\n",
        "skipped_short = 0\n",
        "skipped_empty = 0\n",
        "\n",
        "for filepath in sorted(md_files):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        raw_text = f.read()\n",
        "    \n",
        "    # Skip files with less than 100 symbols in the raw text\n",
        "    if len(raw_text) < MIN_SYMBOLS:\n",
        "        skipped_short += 1\n",
        "        continue\n",
        "    \n",
        "    cleaned = clean_markdown(raw_text)\n",
        "    \n",
        "    # Also skip if the cleaned text is too short\n",
        "    if len(cleaned) < MIN_SYMBOLS:\n",
        "        skipped_empty += 1\n",
        "        continue\n",
        "    \n",
        "    raw_texts.append(raw_text)\n",
        "    cleaned_texts.append(cleaned)\n",
        "\n",
        "print(f\"Kept: {len(cleaned_texts)} files\")\n",
        "print(f\"Skipped (raw < {MIN_SYMBOLS} symbols): {skipped_short}\")\n",
        "print(f\"Skipped (cleaned < {MIN_SYMBOLS} symbols): {skipped_empty}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Inspect cleaning results\n",
        "\n",
        "Let's look at a few examples to verify the cleaning is working correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXAMPLE 1 (length: 3482 chars)\n",
            "================================================================================\n",
            "The Architecture of the Orchestrator\n",
            "\n",
            "The input for the Orchestrator is an Ingestion Plan. As described above, these can be configured in the .\n",
            "For one plan, several items can be configured that are executed together once the overall Ingestion Plan is triggered.\n",
            "For a single item, the following properties need to be defined:\n",
            "Data Connection Name: The One Data Connection to the Source System on which the ingestion should take place.\n",
            "Schema or Domain ID: References either the One Data Domain to in\n",
            "\n",
            "... [truncated, total 3482 chars]\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 2 (length: 3121 chars)\n",
            "================================================================================\n",
            "Archiving and Cleaning Mechanisms of Cartography\n",
            "\n",
            "One Data Cartography has Archiving and Cleaning Mechanisms.\n",
            "As a result of historization processes, different entries get deleted or overwritten. By listing them in archiving tables, entries that are not primarily necessary anymore are saved and archived in a central place, thus keeping the necessary tables clearly arranged. Additionally, as an increasing number of logging messages hinder working with Cartography, these messages get archived from\n",
            "\n",
            "... [truncated, total 3121 chars]\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE 3 (length: 555 chars)\n",
            "================================================================================\n",
            "Cartography Admin App\n",
            "\n",
            "The  is the central place for setting up the Apps in regard to Cartography, especially the Cartography Data Map.\n",
            "Consider the Rights Management in Cartography Article in order to properly set up respective rights to access and configure the Admin App. The following image shows the home screen when entering the Admin App.\n",
            "The following articles explains all the tabs and actions that are offered by the .\n",
            "The name of the Data Map in the header of this App can be adjusted with\n",
            "\n",
            "... [truncated, total 555 chars]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show first 3 cleaned examples\n",
        "for i in range(min(3, len(cleaned_texts))):\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"EXAMPLE {i+1} (length: {len(cleaned_texts[i])} chars)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(cleaned_texts[i][:500])\n",
        "    print(f\"\\n... [truncated, total {len(cleaned_texts[i])} chars]\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Build a Hugging Face Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 255\n",
            "})\n",
            "\n",
            "Total characters: 747,911\n"
          ]
        }
      ],
      "source": [
        "od_dataset = [{'text': text} for text in cleaned_texts]\n",
        "dataset = datasets.Dataset.from_list(od_dataset)\n",
        "print(dataset)\n",
        "print(f\"\\nTotal characters: {sum(len(t) for t in cleaned_texts):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Cleaning\n",
        "\n",
        "Apply the same cleaning steps from the reference notebook:\n",
        "1. Filter out samples that are too short\n",
        "2. Remove repetitions within a single text example\n",
        "3. Remove duplicated documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting dataset size: 255 rows\n"
          ]
        }
      ],
      "source": [
        "print(f\"Starting dataset size: {dataset.num_rows} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Remove examples that are too short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 255/255 [00:00<00:00, 65053.68 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After paragraph length filter: 255 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def paragraph_length_filter(x):\n",
        "    \"\"\"Returns False iff a page has too few lines or lines are too short.\"\"\"\n",
        "    lines = x['text'].split('\\n')\n",
        "    if (\n",
        "        len(lines) < 3\n",
        "        or min(heapq.nlargest(3, [len(line) for line in lines])) < 3\n",
        "    ):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "dataset = dataset.filter(\n",
        "    paragraph_length_filter,\n",
        "    load_from_cache_file=False\n",
        ")\n",
        "print(f\"After paragraph length filter: {dataset.num_rows} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Remove repeated text within training examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 255/255 [00:00<00:00, 25280.63 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After paragraph repetition filter: 230 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def find_duplicates(paragraphs):\n",
        "    \"\"\"\n",
        "    Use this function to find the number of repetitions\n",
        "    in the paragraphs.\n",
        "    \"\"\"\n",
        "    unique_x = set()\n",
        "    duplicate_chars = 0\n",
        "    duplicate_elements = 0\n",
        "    for element in paragraphs:\n",
        "        if element in unique_x:\n",
        "            duplicate_chars += len(element)\n",
        "            duplicate_elements += 1\n",
        "        else:\n",
        "            unique_x.add(element)\n",
        "    return duplicate_elements, duplicate_chars\n",
        "\n",
        "\n",
        "def paragraph_repetition_filter(x):\n",
        "    \"\"\"\n",
        "    Returns False iff a page has too many repetitions.\n",
        "    \"\"\"\n",
        "    text = x['text']\n",
        "    paragraphs = re.compile(r\"\\n{2,}\").split(text.strip())\n",
        "    paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)\n",
        "    if len(paragraphs) == 0:\n",
        "        return False\n",
        "    if paragraphs_duplicates / len(paragraphs) > 0.3:\n",
        "        return False\n",
        "    if len(text) == 0:\n",
        "        return False\n",
        "    if char_duplicates / len(text) > 0.2:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "dataset = dataset.filter(\n",
        "    paragraph_repetition_filter,\n",
        "    load_from_cache_file=False\n",
        ")\n",
        "print(f\"After paragraph repetition filter: {dataset.num_rows} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Deduplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter (num_proc=1): 100%|██████████| 230/230 [00:00<00:00, 2345.03 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After deduplication: 230 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def deduplication(ds):\n",
        "    def dedup_func(x):\n",
        "        \"\"\"Use this function to remove duplicate entries\"\"\"\n",
        "        if x['text'] in unique_text:\n",
        "            return False\n",
        "        else:\n",
        "            unique_text.add(x['text'])\n",
        "            return True\n",
        "\n",
        "    unique_text = set()\n",
        "    ds = ds.filter(dedup_func, load_from_cache_file=False, num_proc=1)\n",
        "    return ds\n",
        "\n",
        "dataset = deduplication(dataset)\n",
        "print(f\"After deduplication: {dataset.num_rows} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Summary of data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final cleaned dataset: 230 rows\n",
            "Total characters: 634,203\n",
            "\n",
            "Text length statistics:\n",
            "  Min:    110 chars\n",
            "  Max:    26,589 chars\n",
            "  Mean:   2,757 chars\n",
            "  Median: 1,811 chars\n"
          ]
        }
      ],
      "source": [
        "print(f\"Final cleaned dataset: {dataset.num_rows} rows\")\n",
        "print(f\"Total characters: {sum(len(t) for t in dataset['text']):,}\")\n",
        "\n",
        "# Show text length distribution\n",
        "lengths = [len(t) for t in dataset['text']]\n",
        "print(f\"\\nText length statistics:\")\n",
        "print(f\"  Min:    {min(lengths):,} chars\")\n",
        "print(f\"  Max:    {max(lengths):,} chars\")\n",
        "print(f\"  Mean:   {np.mean(lengths):,.0f} chars\")\n",
        "print(f\"  Median: {np.median(lengths):,.0f} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Save the preprocessed dataset to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 257.07ba/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved preprocessed dataset to post_training_od_docu_dataset.parquet\n",
            "File size: 283.0 KB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "\n",
        "file_path = \"post_training_od_docu_dataset.parquet\"\n",
        "dataset.to_parquet(file_path)\n",
        "print(f\"Saved preprocessed dataset to {file_path}\")\n",
        "print(f\"File size: {os.path.getsize(file_path) / 1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenization and Packaging\n",
        "\n",
        "### 4.1 Load the preprocessed dataset and tokenize\n",
        "\n",
        "Using the GPT-2 tokenizer (tiktoken) which matches the model in `train_gpt2.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 230 examples [00:00, 23474.06 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 230\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Reload from parquet to verify persistence\n",
        "dataset = datasets.load_dataset(\n",
        "    \"parquet\",\n",
        "    data_files=\"post_training_od_docu_dataset.parquet\",\n",
        "    split=\"train\"\n",
        ")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 50257\n",
            "EOT token id: 50256\n",
            "\n",
            "Example tokenization:\n",
            "  'Hello world' → [15496, 995]\n"
          ]
        }
      ],
      "source": [
        "# Initialize the GPT-2 tokenizer (same as used in train_gpt2.py)\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "eot = enc._special_tokens['<|endoftext|>']  # end of text token\n",
        "\n",
        "print(f\"Vocab size: {enc.n_vocab}\")\n",
        "print(f\"EOT token id: {eot}\")\n",
        "print(f\"\\nExample tokenization:\")\n",
        "print(f\"  'Hello world' → {enc.encode('Hello world')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 230/230 [00:00<00:00, 4742.38 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'input_ids', 'num_tokens'],\n",
            "    num_rows: 230\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def tokenization(example):\n",
        "    \"\"\"Tokenize text and add BOS/EOS tokens.\"\"\"\n",
        "    tokens = enc.encode_ordinary(example[\"text\"])\n",
        "    # Add <|endoftext|> as both BOS and EOS (GPT-2 convention)\n",
        "    token_ids = [eot] + tokens + [eot]\n",
        "    example[\"input_ids\"] = token_ids\n",
        "    example[\"num_tokens\"] = len(token_ids)\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(tokenization, load_from_cache_file=False)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text (first 200 chars): The Architecture of the Orchestrator\n",
            "\n",
            "The input for the Orchestrator is an Ingestion Plan. As described above, these can be configured in the .\n",
            "For one plan, several items can be configured that are e\n",
            "\n",
            "Input IDs (first 30): [50256, 464, 29778, 286, 262, 30369, 2536, 1352, 198, 198, 464, 5128, 329, 262, 30369, 2536, 1352, 318, 281, 554, 3495, 295, 5224, 13, 1081, 3417, 2029, 11, 777, 460]\n",
            "Num tokens: 749\n"
          ]
        }
      ],
      "source": [
        "# Inspect a sample\n",
        "sample = dataset[0]\n",
        "print(\"Text (first 200 chars):\", sample[\"text\"][:200])\n",
        "print(f\"\\nInput IDs (first 30): {sample['input_ids'][:30]}\")\n",
        "print(f\"Num tokens: {sample['num_tokens']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens in dataset: 141,732\n"
          ]
        }
      ],
      "source": [
        "total_tokens = np.sum(dataset[\"num_tokens\"])\n",
        "print(f\"Total tokens in dataset: {total_tokens:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Save as .npy shards (format expected by `DataLoaderLite` in `train_gpt2.py`)\n",
        "\n",
        "`DataLoaderLite` expects flat `.npy` arrays of `uint16` token IDs, with filenames containing `\"train\"` or `\"val\"`. It handles windowing into 1024-length sequences internally — **no pre-packing needed**.\n",
        "\n",
        "This matches the format produced by `fineweb.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 141,732\n",
            "Dtype: uint16\n",
            "  val: 14,174 tokens\n",
            "  train: 127,558 tokens\n"
          ]
        }
      ],
      "source": [
        "# Concatenate all input_ids into a single flat array (same as fineweb.py)\n",
        "all_tokens = np.concatenate(dataset[\"input_ids\"])\n",
        "\n",
        "# Convert to uint16 (GPT-2 vocab is 50257 < 2^16 = 65536, so uint16 is fine)\n",
        "assert (0 <= all_tokens).all() and (all_tokens < 2**16).all(), \"token values out of uint16 range\"\n",
        "all_tokens = all_tokens.astype(np.uint16)\n",
        "\n",
        "print(f\"Total tokens: {len(all_tokens):,}\")\n",
        "print(f\"Dtype: {all_tokens.dtype}\")\n",
        "\n",
        "# Split into train (90%) and val (10%)\n",
        "split_idx = int(len(all_tokens) * 0.9)\n",
        "splits = {\n",
        "    \"val\": all_tokens[split_idx:],\n",
        "    \"train\": all_tokens[:split_idx],\n",
        "}\n",
        "\n",
        "for split_name, tokens in splits.items():\n",
        "    print(f\"  {split_name}: {len(tokens):,} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved ./data/od_docu_val_000000.npy  (14,174 tokens, 27.8 KB)\n",
            "Saved ./data/od_docu_train_000000.npy  (127,558 tokens, 249.3 KB)\n"
          ]
        }
      ],
      "source": [
        "# Save as .npy shards into data/ directory\n",
        "# Naming convention matches fineweb.py: {prefix}_{split}_{shard_index:06d}.npy\n",
        "data_dir = \"./data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "for split_name, tokens in splits.items():\n",
        "    filename = os.path.join(data_dir, f\"od_docu_{split_name}_000000\")\n",
        "    np.save(filename, tokens)\n",
        "    fsize = os.path.getsize(filename + \".npy\") / 1024\n",
        "    print(f\"Saved {filename}.npy  ({len(tokens):,} tokens, {fsize:.1f} KB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "od_docu_train_000000.npy: shape=(127558,), dtype=uint16, first 10 tokens=[50256   464 29778   286   262 30369  2536  1352   198   198]\n",
            "od_docu_val_000000.npy: shape=(14174,), dtype=uint16, first 10 tokens=[ 2099   198 29665   952    12  3351  3021    14 25628    13]\n"
          ]
        }
      ],
      "source": [
        "# Verify the saved shards can be loaded (same way DataLoaderLite does it)\n",
        "for f in sorted(os.listdir(data_dir)):\n",
        "    if f.endswith(\".npy\"):\n",
        "        loaded = np.load(os.path.join(data_dir, f), mmap_mode='r')\n",
        "        print(f\"{f}: shape={loaded.shape}, dtype={loaded.dtype}, first 10 tokens={loaded[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary\n",
        "\n",
        "The `.npy` shards in `./data/` are ready for post-training. To use them with `train_gpt2.py`, set:\n",
        "```python\n",
        "data_root = \"post-training/data\"  # or the appropriate relative/absolute path\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DATASET SUMMARY\n",
            "============================================================\n",
            "Source .md files:   255\n",
            "After cleaning:     255 documents\n",
            "After filters:      230 documents\n",
            "Total tokens:       141,732\n",
            "  Train tokens:     127,558 (90%)\n",
            "  Val tokens:       14,174 (10%)\n",
            "Output directory:   /Users/alexander.hafer/Desktop/LLM/Karpathy/reproduce_gpt-2/post-training/data\n",
            "Output files:\n",
            "  od_docu_train_000000.npy (249.3 KB)\n",
            "  od_docu_val_000000.npy (27.8 KB)\n",
            "\n",
            "Ready for post-training with train_gpt2.py!\n"
          ]
        }
      ],
      "source": [
        "print(f\"{'='*60}\")\n",
        "print(f\"DATASET SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Source .md files:   {len(md_files)}\")\n",
        "print(f\"After cleaning:     {len(cleaned_texts)} documents\")\n",
        "print(f\"After filters:      {dataset.num_rows} documents\")\n",
        "print(f\"Total tokens:       {total_tokens:,}\")\n",
        "print(f\"  Train tokens:     {len(splits['train']):,} (90%)\")\n",
        "print(f\"  Val tokens:       {len(splits['val']):,} (10%)\")\n",
        "print(f\"Output directory:   {os.path.abspath(data_dir)}\")\n",
        "print(f\"Output files:\")\n",
        "for f in sorted(os.listdir(data_dir)):\n",
        "    if f.endswith(\".npy\"):\n",
        "        print(f\"  {f} ({os.path.getsize(os.path.join(data_dir, f)) / 1024:.1f} KB)\")\n",
        "print(f\"\\nReady for post-training with train_gpt2.py!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed intermediate file: post_training_od_docu_dataset.parquet\n"
          ]
        }
      ],
      "source": [
        "# Cleanup: remove the intermediate parquet file (no longer needed)\n",
        "parquet_file = \"post_training_od_docu_dataset.parquet\"\n",
        "if os.path.exists(parquet_file):\n",
        "    os.remove(parquet_file)\n",
        "    print(f\"Removed intermediate file: {parquet_file}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
